<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Overload Backpressure Answers</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <link rel="stylesheet" href="../../../assets/styles.css">
</head>
<body class="antialiased">
    <header class="bg-white sticky top-0 z-50 shadow-md">
        <nav class="container mx-auto px-4 sm:px-6 py-4 flex justify-between items-center">
            <div class="text-xl sm:text-2xl font-bold text-gray-800">
                <a href="../../../../index.html" class="flex items-center">
                    <svg class="w-6 h-6 mr-2" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    <span class="gradient-text">Home</span>
                </a>
            </div>
            <ul class="flex space-x-4 sm:space-x-6 text-gray-600 font-medium text-sm sm:text-base">
                <li><a href="../../../../case_studies.html" class="hover:text-[#bc5090] font-semibold">Case Studies</a></li>
                <li><a href="../../../../aboutme.html" class="hover:text-[#bc5090] font-semibold">About Me</a></li>
            </ul>
        </nav>
    </header>
        <button class="sidebar-toggle mobile" id="sidebarToggleMobile" aria-label="Toggle sidebar">
        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
        </svg>
    </button>
    <button class="sidebar-toggle desktop" id="sidebarToggleDesktop" aria-label="Toggle sidebar">
        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
        </svg>
    </button>
        <div class="sidebar-overlay" id="sidebarOverlay"></div>
    <aside class="sidebar" id="sidebar">
        <div class="sidebar-header">
            <div class="sidebar-title">Table of Contents</div>
            <button class="sidebar-close" id="sidebarClose" aria-label="Close sidebar">
                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path>
                </svg>
            </button>
        </div>
        <div class="sidebar-content">
            <ul class="sidebar-nav">
                <li><a href="#answer-key-overload-backpressure" class="level-1">Answer Key: Overload &amp; Backpressure</a></li>
                <li><a href="#exercise-1-design-backpressure" class="level-2">Exercise 1: Design Backpressure</a></li>
                <li><a href="#answer" class="level-3">Answer</a></li>
                <li><a href="#design-components" class="level-3">Design Components</a></li>
                <li><a href="#complete-design" class="level-3">Complete Design</a></li>
                <li><a href="#exercise-2-prevent-cascades" class="level-2">Exercise 2: Prevent Cascades</a></li>
                <li><a href="#answer_1" class="level-3">Answer</a></li>
                <li><a href="#prevention-mechanisms" class="level-3">Prevention Mechanisms</a></li>
                <li><a href="#complete-solution" class="level-3">Complete Solution</a></li>
                <li><a href="#exercise-3-load-shedding-strategy" class="level-2">Exercise 3: Load Shedding Strategy</a></li>
                <li><a href="#answer_2" class="level-3">Answer</a></li>
                <li><a href="#request-classification" class="level-3">Request Classification</a></li>
                <li><a href="#load-shedding-strategy" class="level-3">Load Shedding Strategy</a></li>
                <li><a href="#implementation" class="level-3">Implementation</a></li>
                <li><a href="#response-strategy" class="level-3">Response Strategy</a></li>
                <li><a href="#answer_3" class="level-3">Answer</a></li>
            </ul>
        </div>
    </aside>
    <main class="main-content container mx-auto px-4 sm:px-6 lg:px-8 py-12">
        <div class="content">
<h1 id="answer-key-overload-backpressure">Answer Key: Overload &amp; Backpressure</h1>
<p><a href="../../02-distributed-systems/overload-backpressure.html">Back to Exercises</a></p>
<hr />
<h2 id="exercise-1-design-backpressure">Exercise 1: Design Backpressure</h2>
<p><strong>Question</strong>: Design a system that handles 10× load gracefully. What mechanisms do you use?</p>
<h3 id="answer">Answer</h3>
<p><strong>Goal</strong>: Handle 10× normal load without complete failure.</p>
<h3 id="design-components">Design Components</h3>
<p><strong>1. Load Detection</strong></p>
<p><strong>Metrics to monitor</strong>:
- Queue depth (alert when &gt; 2× normal)
- Latency (alert when P95 &gt; 2× normal)
- Error rate (alert when &gt; 2× normal)
- Resource utilization (alert when &gt; 80%)</p>
<p><strong>Detection thresholds</strong>:
- <strong>Normal load</strong>: Baseline metrics
- <strong>2× load</strong>: Warning threshold
- <strong>5× load</strong>: Critical threshold
- <strong>10× load</strong>: Emergency threshold</p>
<p><strong>2. Auto-Scaling</strong></p>
<p><strong>Horizontal scaling</strong>:
- <strong>Min replicas</strong>: 3 (for redundancy)
- <strong>Max replicas</strong>: 30 (10× normal capacity)
- <strong>Scale up</strong>: Add 2 pods when CPU &gt; 70% or queue depth &gt; threshold
- <strong>Scale down</strong>: Remove 1 pod when CPU &lt; 50% and queue depth &lt; threshold
- <strong>Cooldown</strong>: 2 minutes between scaling events</p>
<p><strong>Scaling strategy</strong>:
- Scale aggressively (add capacity quickly)
- Scale conservatively (remove capacity slowly)
- Pre-scale for known traffic patterns</p>
<p><strong>3. Load Shedding</strong></p>
<p><strong>When to shed load</strong>:
- Queue depth &gt; threshold (e.g., &gt; 1000 requests)
- Latency &gt; threshold (e.g., P95 &gt; 500ms)
- Resource utilization &gt; 90%</p>
<p><strong>Load shedding strategies</strong>:
- <strong>Random drop</strong>: Drop random requests (simple but unfair)
- <strong>Priority-based</strong>: Drop low-priority requests first (better UX)
- <strong>Client-based</strong>: Drop requests from specific clients (protect important clients)
- <strong>Request type</strong>: Drop read requests before write requests (preserve data integrity)</p>
<p><strong>Implementation</strong>:
- Drop requests at load balancer or API gateway
- Return HTTP 429 (Too Many Requests)
- Include Retry-After header</p>
<p><strong>4. Circuit Breakers</strong></p>
<p><strong>Purpose</strong>: Stop calling downstream services when they're failing.</p>
<p><strong>Configuration</strong>:
- <strong>Failure threshold</strong>: 50% failure rate
- <strong>Timeout</strong>: 5 seconds
- <strong>Half-open interval</strong>: 30 seconds
- <strong>Success threshold</strong>: 3 successful requests</p>
<p><strong>Behavior</strong>:
- <strong>Closed</strong>: Normal operation, calls downstream
- <strong>Open</strong>: Fails fast, doesn't call downstream
- <strong>Half-open</strong>: Test if downstream recovered</p>
<p><strong>5. Rate Limiting</strong></p>
<p><strong>Per-client rate limits</strong>:
- Normal clients: 100 requests/second
- Premium clients: 1000 requests/second
- Anonymous clients: 10 requests/second</p>
<p><strong>Global rate limits</strong>:
- Total system capacity: 10,000 QPS
- When exceeded: Return 429, queue requests</p>
<p><strong>6. Graceful Degradation</strong></p>
<p><strong>Feature flags</strong>:
- Disable non-critical features under load
- Reduce functionality to core features only
- Return cached data instead of fresh data</p>
<p><strong>Response strategies</strong>:
- <strong>Fast path</strong>: Serve cached data, skip expensive operations
- <strong>Reduced functionality</strong>: Disable optional features
- <strong>Timeout reduction</strong>: Reduce timeouts to fail fast</p>
<p><strong>7. Monitoring &amp; Alerting</strong></p>
<p><strong>Metrics</strong>:
- Request rate, queue depth, latency, error rate
- Resource utilization (CPU, memory, I/O)
- Scaling events, load shedding events</p>
<p><strong>Alerts</strong>:
- <strong>Warning</strong>: 2× load detected
- <strong>Critical</strong>: 5× load detected
- <strong>Emergency</strong>: 10× load detected</p>
<h3 id="complete-design">Complete Design</h3>
<div class="mermaid">
graph TD
Load[10× Load] --> Detect[Load Detection]
Detect --> Scale{Auto-Scale?}
Scale -->|Yes| ScaleUp[Scale Up]
Scale -->|No| Shed{Load Shed?}
Shed -->|Yes| Drop[Drop Requests]
Shed -->|No| Circuit{Circuit Break?}
Circuit -->|Yes| FailFast[Fail Fast]
Circuit -->|No| Degrade[Graceful Degradation]
ScaleUp --> Monitor[Monitor]
Drop --> Monitor
FailFast --> Monitor
Degrade --> Monitor
style Load fill:#ff9999
style Detect fill:#ffcc99
style Monitor fill:#99ff99
</div>
<p><strong>Answer</strong>: <strong>Multi-layered approach</strong>:</p>
<ol>
<li><strong>Auto-scaling</strong>: Scale to 10× capacity (30 replicas)</li>
<li><strong>Load shedding</strong>: Drop low-priority requests when queue depth &gt; threshold</li>
<li><strong>Circuit breakers</strong>: Stop calling failing downstream services</li>
<li><strong>Rate limiting</strong>: Limit per-client and global rates</li>
<li><strong>Graceful degradation</strong>: Disable non-critical features</li>
<li><strong>Monitoring</strong>: Track metrics and alert on thresholds</li>
</ol>
<p><strong>Key principles</strong>:
- <strong>Fail gracefully</strong>: Better to serve some requests than none
- <strong>Protect core</strong>: Preserve critical functionality
- <strong>Fail fast</strong>: Don't wait indefinitely
- <strong>Monitor everything</strong>: Know what's happening</p>
<hr />
<h2 id="exercise-2-prevent-cascades">Exercise 2: Prevent Cascades</h2>
<p><strong>Question</strong>: Service A calls Service B. How do you prevent B's failure from cascading to A?</p>
<h3 id="answer_1">Answer</h3>
<p><strong>Problem</strong>: When Service B fails, Service A may also fail, creating a cascade.</p>
<h3 id="prevention-mechanisms">Prevention Mechanisms</h3>
<p><strong>1. Circuit Breaker</strong></p>
<p><strong>Purpose</strong>: Stop calling Service B when it's failing.</p>
<p><strong>Configuration</strong>:
- <strong>Failure threshold</strong>: 50% failure rate over 10 requests
- <strong>Timeout</strong>: 2 seconds (fail fast)
- <strong>Half-open interval</strong>: 30 seconds
- <strong>Success threshold</strong>: 3 successful requests to close circuit</p>
<p><strong>Behavior</strong>:
- <strong>Closed</strong>: Normal operation, calls Service B
- <strong>Open</strong>: Fails fast, returns error immediately (doesn't call Service B)
- <strong>Half-open</strong>: Test if Service B recovered</p>
<p><strong>Implementation</strong>:</p>
<pre><code class="language-python">if circuit_breaker.is_open():
    return error(&quot;Service B unavailable&quot;)
else:
    try:
        result = call_service_b()
        circuit_breaker.record_success()
        return result
    except Exception as e:
        circuit_breaker.record_failure()
        raise
</code></pre>
<p><strong>2. Timeouts</strong></p>
<p><strong>Purpose</strong>: Don't wait indefinitely for Service B.</p>
<p><strong>Configuration</strong>:
- <strong>Connection timeout</strong>: 1 second
- <strong>Request timeout</strong>: 2 seconds
- <strong>Total timeout</strong>: 3 seconds</p>
<p><strong>Why important</strong>:
- Prevents Service A from hanging
- Fails fast instead of waiting
- Reduces resource usage</p>
<p><strong>3. Retry Limits</strong></p>
<p><strong>Purpose</strong>: Limit retries to prevent amplifying load on Service B.</p>
<p><strong>Configuration</strong>:
- <strong>Max retries</strong>: 2 (total 3 attempts)
- <strong>Exponential backoff</strong>: 100ms, 200ms, 400ms
- <strong>Retry only on</strong>: Transient errors (5xx, timeouts)
- <strong>Don't retry on</strong>: Client errors (4xx)</p>
<p><strong>Why important</strong>:
- Prevents retry storms
- Reduces load on failing service
- Fails fast after retries exhausted</p>
<p><strong>4. Bulkhead Pattern</strong></p>
<p><strong>Purpose</strong>: Isolate failures to prevent resource exhaustion.</p>
<p><strong>Implementation</strong>:
- <strong>Separate thread pools</strong>: One for Service B calls, one for other operations
- <strong>Resource limits</strong>: Limit resources used for Service B calls
- <strong>Isolation</strong>: Failure in Service B doesn't affect other operations</p>
<p><strong>5. Fallback Mechanisms</strong></p>
<p><strong>Purpose</strong>: Provide alternative behavior when Service B fails.</p>
<p><strong>Options</strong>:
- <strong>Cached data</strong>: Return cached response from Service B
- <strong>Default values</strong>: Return sensible defaults
- <strong>Degraded mode</strong>: Reduce functionality but continue operating
- <strong>Error response</strong>: Return error but don't crash</p>
<p><strong>6. Load Shedding</strong></p>
<p><strong>Purpose</strong>: Reduce load on Service A to prevent cascade.</p>
<p><strong>When</strong>: Service B is failing, reduce load on Service A:
- Drop low-priority requests
- Reduce request rate
- Return errors for non-critical requests</p>
<p><strong>7. Monitoring &amp; Alerting</strong></p>
<p><strong>Purpose</strong>: Detect failures early.</p>
<p><strong>Metrics</strong>:
- Service B error rate
- Service B latency
- Circuit breaker state
- Retry rate</p>
<p><strong>Alerts</strong>:
- Service B error rate &gt; threshold
- Circuit breaker opened
- High retry rate</p>
<h3 id="complete-solution">Complete Solution</h3>
<div class="mermaid">
sequenceDiagram
participant A as Service A
participant CB as Circuit Breaker
participant B as Service B
A->>CB: Check circuit state
alt Circuit Open
CB-->>A: Fail fast (don't call B)
else Circuit Closed
A->>B: Call with timeout
alt Success
B-->>A: Response
A->>CB: Record success
else Failure
B-->>A: Error/Timeout
A->>CB: Record failure
alt Retry limit not exceeded
A->>B: Retry with backoff
else Retry limit exceeded
A->>CB: Open circuit
A->>A: Use fallback
end
end
end
</div>
<p><strong>Answer</strong>: <strong>Multi-layered defense</strong>:</p>
<ol>
<li><strong>Circuit breaker</strong>: Stop calling Service B when it's failing</li>
<li><strong>Timeouts</strong>: Don't wait indefinitely (fail fast)</li>
<li><strong>Retry limits</strong>: Limit retries (max 2 retries)</li>
<li><strong>Exponential backoff</strong>: Space out retries</li>
<li><strong>Bulkhead</strong>: Isolate Service B calls (separate thread pool)</li>
<li><strong>Fallback</strong>: Use cached data or defaults when Service B fails</li>
<li><strong>Load shedding</strong>: Reduce load on Service A if Service B failing</li>
<li><strong>Monitoring</strong>: Alert on Service B failures</li>
</ol>
<p><strong>Key principles</strong>:
- <strong>Fail fast</strong>: Don't wait for failing service
- <strong>Isolate failures</strong>: Prevent resource exhaustion
- <strong>Provide fallbacks</strong>: Continue operating when possible
- <strong>Monitor everything</strong>: Detect failures early</p>
<hr />
<h2 id="exercise-3-load-shedding-strategy">Exercise 3: Load Shedding Strategy</h2>
<p><strong>Question</strong>: Design a load shedding strategy for an API that handles both read and write requests. Which requests do you drop first?</p>
<h3 id="answer_2">Answer</h3>
<p><strong>Goal</strong>: Drop requests when overloaded, prioritizing important requests.</p>
<h3 id="request-classification">Request Classification</h3>
<p><strong>1. Request Types</strong>:
- <strong>Read requests</strong>: GET requests, data retrieval
- <strong>Write requests</strong>: POST, PUT, DELETE requests, data modification</p>
<p><strong>2. Request Priority</strong>:
- <strong>Critical writes</strong>: Must succeed (payments, orders)
- <strong>Normal writes</strong>: Should succeed (updates, creates)
- <strong>Critical reads</strong>: Must succeed (auth checks, critical data)
- <strong>Normal reads</strong>: Can be dropped (cached data, non-critical)</p>
<p><strong>3. Client Classification</strong>:
- <strong>Premium clients</strong>: High priority
- <strong>Normal clients</strong>: Normal priority
- <strong>Anonymous clients</strong>: Low priority</p>
<h3 id="load-shedding-strategy">Load Shedding Strategy</h3>
<p><strong>Priority Order</strong> (drop in this order):</p>
<p><strong>1. Anonymous read requests</strong> (lowest priority)
- <strong>Why</strong>: Anonymous users, can retry
- <strong>Impact</strong>: Low (users can retry)
- <strong>When</strong>: Drop when queue depth &gt; 500</p>
<p><strong>2. Normal read requests from normal clients</strong>
- <strong>Why</strong>: Can be cached, less critical
- <strong>Impact</strong>: Medium (users may see stale data)
- <strong>When</strong>: Drop when queue depth &gt; 1000</p>
<p><strong>3. Normal write requests from normal clients</strong>
- <strong>Why</strong>: Less critical than reads, can retry
- <strong>Impact</strong>: Medium (users may need to retry)
- <strong>When</strong>: Drop when queue depth &gt; 1500</p>
<p><strong>4. Critical read requests from normal clients</strong>
- <strong>Why</strong>: Important but can retry
- <strong>Impact</strong>: High (users may be affected)
- <strong>When</strong>: Drop when queue depth &gt; 2000</p>
<p><strong>5. Normal write requests from premium clients</strong>
- <strong>Why</strong>: Premium clients but non-critical writes
- <strong>Impact</strong>: High (premium users affected)
- <strong>When</strong>: Drop when queue depth &gt; 2500</p>
<p><strong>6. Critical write requests from normal clients</strong>
- <strong>Why</strong>: Critical but from normal clients
- <strong>Impact</strong>: Very high (critical operations affected)
- <strong>When</strong>: Drop when queue depth &gt; 3000</p>
<p><strong>7. Critical read requests from premium clients</strong>
- <strong>Why</strong>: Critical and premium
- <strong>Impact</strong>: Very high
- <strong>When</strong>: Drop when queue depth &gt; 3500</p>
<p><strong>8. Critical write requests from premium clients</strong> (never drop)
- <strong>Why</strong>: Most critical, never drop
- <strong>Impact</strong>: Critical (payments, orders)
- <strong>When</strong>: Never drop (serve even if overloaded)</p>
<h3 id="implementation">Implementation</h3>
<p><strong>Load shedding algorithm</strong>:</p>
<pre><code class="language-python">def should_drop_request(request):
    queue_depth = get_queue_depth()

    # Never drop critical writes from premium clients
    if request.is_critical_write() and request.is_premium():
        return False

    # Drop based on priority and queue depth
    priority = request.get_priority()
    threshold = get_threshold_for_priority(priority)

    return queue_depth &gt; threshold
</code></pre>
<p><strong>Thresholds</strong>:
- Anonymous reads: 500
- Normal reads: 1000
- Normal writes: 1500
- Critical reads: 2000
- Premium normal writes: 2500
- Critical writes: 3000
- Premium critical reads: 3500
- Premium critical writes: Never</p>
<h3 id="response-strategy">Response Strategy</h3>
<p><strong>When dropping requests</strong>:
- Return HTTP 429 (Too Many Requests)
- Include Retry-After header (e.g., 5 seconds)
- Include error message explaining why
- Log dropped requests for analysis</p>
<p><strong>Monitoring</strong>:
- Track dropped requests by type
- Monitor queue depth
- Alert on high drop rate
- Analyze drop patterns</p>
<h3 id="answer_3">Answer</h3>
<p><strong>Load shedding priority</strong> (drop in order):</p>
<ol>
<li><strong>Anonymous read requests</strong> (drop at queue depth &gt; 500)</li>
<li><strong>Normal read requests</strong> (drop at queue depth &gt; 1000)</li>
<li><strong>Normal write requests</strong> (drop at queue depth &gt; 1500)</li>
<li><strong>Critical read requests</strong> (drop at queue depth &gt; 2000)</li>
<li><strong>Premium normal write requests</strong> (drop at queue depth &gt; 2500)</li>
<li><strong>Critical write requests</strong> (drop at queue depth &gt; 3000)</li>
<li><strong>Premium critical read requests</strong> (drop at queue depth &gt; 3500)</li>
<li><strong>Premium critical write requests</strong> (<strong>never drop</strong>)</li>
</ol>
<p><strong>Key principles</strong>:
- <strong>Preserve data integrity</strong>: Never drop critical writes
- <strong>Prioritize premium clients</strong>: Drop normal clients first
- <strong>Reads before writes</strong>: Drop reads before writes (writes are harder to retry)
- <strong>Gradual degradation</strong>: Drop less important requests first
- <strong>Monitor and adjust</strong>: Track drops and adjust thresholds</p>
<p><strong>Rationale</strong>:
- <strong>Writes are harder to retry</strong>: Users may have already acted
- <strong>Reads can be cached</strong>: Stale data is better than no data
- <strong>Premium clients pay more</strong>: Should get better service
- <strong>Critical operations</strong>: Must succeed (payments, orders)</p>
        </div>
    </main>
    <footer class="bg-gray-800 text-white text-center p-6 mt-16">
        <p>&copy; 2025 Data Engineering Guides. An illustrative web application.</p>
    </footer>
    <script src="../../../assets/scripts.js"></script>
    
</body>
</html>
