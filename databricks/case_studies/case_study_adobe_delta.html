<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case Study: Adobe's Data Platform with Delta Lake</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F0F4F8;
            color: #1E293B;
        }
        .gradient-text {
            background: linear-gradient(90deg, #FF3621, #FF6150);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .card {
            background-color: white;
            border-radius: 0.75rem;
            padding: 1.5rem;
            box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.05), 0 4px 6px -4px rgb(0 0 0 / 0.05);
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@8.13.8/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
</head>
<body class="antialiased">
    <header class="bg-white sticky top-0 z-50 shadow-md">
        <nav class="container mx-auto px-4 sm:px-6 py-4 flex justify-between items-center">
            <div class="text-xl sm:text-2xl font-bold text-gray-800">
                <a href="index.html" class="gradient-text">Back to Databricks Case Studies</a>
            </div>
            <ul class="flex space-x-4 sm:space-x-6 text-gray-600 font-medium text-sm sm:text-base">
                <li><a href="../../index.html" class="hover:text-[#FF6150]">Home</a></li>
                <li><a href="../../aboutme.html" class="hover:text-[#FF6150] font-semibold">About Me</a></li>
            </ul>
        </nav>
    </header>
    <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-12">
        <section id="hero" class="text-center my-12 sm:my-20">
            <h1 class="text-3xl sm:text-4xl md:text-5xl lg:text-6xl font-black mb-4 leading-tight">
                Case Study: Massive Data Processing in Adobe Experience Platform with Delta Lake
            </h1>
            <p class="text-base sm:text-lg text-gray-600 max-w-3xl mx-auto">How Adobe built a cost-effective and scalable data pipeline using Delta Lake and Apache Spark to power its Experience Platform.</p>
        </section>
        <div class="space-y-8">
            <div class="card">
                <h2 class="text-2xl font-bold text-gray-800 mb-4">The Challenge: A Multi-Tenant, Petabyte-Scale Platform</h2>
                <p class="text-gray-600">The Adobe Experience Platform unifies data from across Adobe's products to power real-time customer profiles and AI/ML services. The platform needed to:</p>
                <ul class="list-disc list-inside mt-4 space-y-2">
                    <li>Process petabytes of data daily from thousands of tenants.</li>
                    <li>Ensure data quality and reliability for both streaming and batch data.</li>
                    <li>Provide a scalable and cost-effective solution for data transformation and enrichment.</li>
                    <li>Handle the "small file problem" caused by ingesting data from many different sources.</li>
                </ul>
            </div>
            <div class="card">
                <h2 class="text-2xl font-bold text-gray-800 mb-4">The Architecture: A Lakehouse Built on Delta Lake</h2>
                <p class="text-gray-600 mb-4">Adobe's solution is a classic Lakehouse architecture, with Delta Lake and Apache Spark on Databricks at its core.</p>
                <div class="mermaid text-center">
graph TD
    subgraph "Data Ingestion"
        A[Streaming Sources] --> B(Kafka);
        C[Batch Sources] --> D[ADLS Gen2];
        B & D --> E{Databricks Ingestion Jobs};
    end
    subgraph "Lakehouse Processing"
        E --> F[Bronze Delta Tables];
        F --> G(ETL with Spark & Delta);
        G --> H[Silver Delta Tables];
        H --> I(Data Enrichment & Aggregation);
        I --> J[Gold Delta Tables];
    end
    subgraph "Serving Layer"
        J --> K(Adobe Experience Platform Services);
        J --> L(BI & Reporting);
    end
                </div>
                <ol class="list-decimal list-inside space-y-3 mt-4">
                    <li><strong>Ingestion:</strong> Streaming and batch data is ingested into a landing zone.</li>
                    <li><strong>Bronze Layer:</strong> The raw data is ingested into Bronze Delta tables, providing an immutable, versioned copy of the source data.</li>
                    <li><strong>Silver Layer:</strong> ETL jobs running on Databricks read from the Bronze tables, perform cleaning, filtering, and schema enforcement, and write the results to Silver Delta tables.</li>
                    <li><strong>Gold Layer:</strong> The Silver data is further enriched, aggregated, and joined to create business-level Gold tables that are optimized for specific use cases, such as powering the real-time customer profile.</li>
                </ol>
            </div>
            <div class="card">
                <h2 class="text-2xl font-bold text-gray-800 mb-4">Key Technical Details & Learnings</h2>
                <ul class="list-disc list-inside mt-4 space-y-3">
                    <li><strong>ACID Transactions with Delta Lake:</strong> Delta Lake's ACID properties were critical for ensuring data reliability and consistency, especially in a multi-tenant environment with concurrent reads and writes.</li>
                    <li><strong>Solving the Small File Problem:</strong> Delta Lake's `OPTIMIZE` command, particularly the `ZORDER` option, was used to compact small files into larger ones and co-locate related information, which significantly improved query performance.</li>
                    <li><strong>Time Travel for Debugging and Auditing:</strong> The ability to query previous versions of a Delta table (Time Travel) was invaluable for debugging data issues and for auditing data changes over time.</li>
                    <li><strong>Scalability with Databricks:</strong> The elastic nature of Databricks clusters allowed Adobe to scale their processing resources up and down to meet the demands of their petabyte-scale workloads, optimizing for both performance and cost.</li>
                </ul>
            </div>
        </div>
    </main>
    <footer class="bg-gray-800 text-white text-center p-6 mt-16">
        <p>&copy; 2025 Data Engineering Guides. An illustrative web application.</p>
    </footer>
</body>
</html>