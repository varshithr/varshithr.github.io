<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Region API on GCP</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0KOVEMmgEkDaBLEqcMW5uud
jLPMdWTocqpoLBTRPtcDGECroCvEOZE" crossorigin="anonymous">
    <link rel="stylesheet" href="../learn/css/learn.css">
</head>
<body class="antialiased">
    <header class="bg-white sticky top-0 z-50 shadow-md">
        <nav class="container mx-auto px-4 sm:px-6 py-4 flex justify-between items-center">
            <div class="text-xl sm:text-2xl font-bold text-gray-800">
                <a href="../index.html" class="flex items-center hover:text-gray-600">
                    <svg class="w-6 h-6 mr-2" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    <span>Data Engineering Concepts</span>
                </a>
            </div>
            <ul class="flex space-x-4 sm:space-x-6 text-gray-600 font-medium text-sm sm:text-base">
                <li><a href="../aboutme.html" class="hover:text-[#bc5090] font-semibold">About Me</a></li>
            </ul>
        </nav>
    </header>
    <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-12">
        <div class="main-content">
            <h1 id="multi-region-api-on-gcp">Multi-Region API on
            GCP</h1>
            <p><strong>One-line summary</strong>: End-to-end design of a
            globally distributed API on GCP with SLOs, rollout strategy,
            and incident playbooks.</p>
            <p><strong>Prerequisites</strong>: <a
            href="../03-gcp-core-building-blocks/README.html">GCP Core
            Building Blocks</a>, <a
            href="../04-reliability-sre/README.html">Reliability &amp;
            SRE</a>, <a
            href="../02-distributed-systems/README.html">Distributed
            Systems</a>.</p>
            <hr />
            <h2 id="system-overview">System Overview</h2>
            <h3 id="requirements">Requirements</h3>
            <p><strong>Functional Requirements</strong>: - REST API for
            user data management - Support 100M+ users globally - Handle
            100K+ QPS peak load - Support read and write operations</p>
            <p><strong>Non-Functional Requirements</strong>: -
            <strong>Latency</strong>: P95 &lt; 100ms, P99 &lt; 200ms -
            <strong>Availability</strong>: 99.9% (SLO) -
            <strong>Consistency</strong>: Strong consistency for writes,
            eventual for reads - <strong>Durability</strong>: 99.999999%
            (11 nines)</p>
            <p><strong>Constraints</strong>: - Must use GCP services -
            Multi-region deployment required - Budget: Optimize for cost
            efficiency</p>
            <hr />
            <h2 id="architecture">Architecture</h2>
            <h3 id="high-level-architecture">High-Level
            Architecture</h3>
            <pre class="mermaid"><code>graph TB
    Users[Users] --&gt; CDN[Cloud CDN]
    CDN --&gt; LB[Global Load Balancer]
    LB --&gt;|Region 1| API1[API Servers&lt;br/&gt;us-central1]
    LB --&gt;|Region 2| API2[API Servers&lt;br/&gt;europe-west1]
    LB --&gt;|Region 3| API3[API Servers&lt;br/&gt;asia-east1]

    API1 --&gt; Cache1[Cloud Memorystore&lt;br/&gt;Redis]
    API2 --&gt; Cache2[Cloud Memorystore&lt;br/&gt;Redis]
    API3 --&gt; Cache3[Cloud Memorystore&lt;br/&gt;Redis]

    API1 --&gt; DB[(Cloud Spanner&lt;br/&gt;Multi-Region)]
    API2 --&gt; DB
    API3 --&gt; DB

    API1 --&gt; PubSub[Pub/Sub&lt;br/&gt;Events]
    API2 --&gt; PubSub
    API3 --&gt; PubSub

    style LB fill:#ffcc99
    style DB fill:#99ccff
    style PubSub fill:#99ff99</code></pre>
            <h3 id="component-details">Component Details</h3>
            <h4 id="global-load-balancer">1. Global Load Balancer</h4>
            <ul>
            <li><strong>Type</strong>: HTTP(S) Global Load Balancer</li>
            <li><strong>SSL</strong>: Managed SSL certificates</li>
            <li><strong>Routing</strong>: Geographic routing (closest
            region)</li>
            <li><strong>Health checks</strong>: HTTP health checks every
            10s</li>
            </ul>
            <h4 id="api-servers">2. API Servers</h4>
            <ul>
            <li><strong>Platform</strong>: GKE (Google Kubernetes
            Engine)</li>
            <li><strong>Regions</strong>: us-central1, europe-west1,
            asia-east1</li>
            <li><strong>Auto-scaling</strong>: Horizontal Pod Autoscaler
            (HPA)</li>
            <li><strong>Replicas</strong>: 3 per region minimum, scale
            to 50 per region</li>
            </ul>
            <h4 id="database">3. Database</h4>
            <ul>
            <li><strong>Service</strong>: Cloud Spanner
            (Multi-Region)</li>
            <li><strong>Configuration</strong>: Multi-region (us, eu,
            asia)</li>
            <li><strong>Replication</strong>: Synchronous replication
            across regions</li>
            <li><strong>Consistency</strong>: External consistency
            (strong)</li>
            </ul>
            <h4 id="cache">4. Cache</h4>
            <ul>
            <li><strong>Service</strong>: Cloud Memorystore (Redis)</li>
            <li><strong>Deployment</strong>: Regional (one per
            region)</li>
            <li><strong>TTL</strong>: 5 minutes for reads</li>
            <li><strong>Eviction</strong>: LRU (Least Recently
            Used)</li>
            </ul>
            <h4 id="event-streaming">5. Event Streaming</h4>
            <ul>
            <li><strong>Service</strong>: Pub/Sub</li>
            <li><strong>Use case</strong>: Async processing,
            notifications</li>
            <li><strong>Topics</strong>: user-events, notifications</li>
            </ul>
            <hr />
            <h2 id="data-flow">Data Flow</h2>
            <h3 id="read-request-flow">Read Request Flow</h3>
            <pre class="mermaid"><code>sequenceDiagram
    participant User
    participant CDN
    participant LB
    participant API
    participant Cache
    participant Spanner

    User-&gt;&gt;CDN: GET /users/123
    CDN-&gt;&gt;LB: Request (if cache miss)
    LB-&gt;&gt;API: Route to nearest region
    API-&gt;&gt;Cache: Check cache
    alt Cache hit
        Cache--&gt;&gt;API: Return cached data
    else Cache miss
        API-&gt;&gt;Spanner: Read from database
        Spanner--&gt;&gt;API: Return data
        API-&gt;&gt;Cache: Store in cache
    end
    API--&gt;&gt;LB: Response
    LB--&gt;&gt;CDN: Response
    CDN--&gt;&gt;User: Response (cached)</code></pre>
            <h3 id="write-request-flow">Write Request Flow</h3>
            <pre class="mermaid"><code>sequenceDiagram
    participant User
    participant LB
    participant API
    participant Spanner
    participant PubSub
    participant Cache

    User-&gt;&gt;LB: PUT /users/123
    LB-&gt;&gt;API: Route to nearest region
    API-&gt;&gt;Spanner: Write (strong consistency)
    Spanner--&gt;&gt;API: Write confirmed
    API-&gt;&gt;Cache: Invalidate cache
    API-&gt;&gt;PubSub: Publish event
    API--&gt;&gt;LB: Success response
    LB--&gt;&gt;User: Success response</code></pre>
            <hr />
            <h2 id="slis-slos-error-budgets">SLIs, SLOs &amp; Error
            Budgets</h2>
            <h3 id="slis-service-level-indicators">SLIs (Service Level
            Indicators)</h3>
            <h4 id="availability-sli">1. Availability SLI</h4>
            <ul>
            <li><strong>Definition</strong>: Fraction of successful HTTP
            requests (2xx, 3xx)</li>
            <li><strong>Measurement</strong>: Count of successful
            requests / total requests</li>
            <li><strong>Current</strong>: 99.95%</li>
            <li><strong>Target</strong>: 99.9% (SLO)</li>
            </ul>
            <h4 id="latency-sli">2. Latency SLI</h4>
            <ul>
            <li><strong>Definition</strong>: P95 and P99 request
            latency</li>
            <li><strong>Measurement</strong>: Time from request received
            to response sent</li>
            <li><strong>Current</strong>: P95=80ms, P99=150ms</li>
            <li><strong>Target</strong>: P95 &lt; 100ms, P99 &lt; 200ms
            (SLO)</li>
            </ul>
            <h4 id="error-rate-sli">3. Error Rate SLI</h4>
            <ul>
            <li><strong>Definition</strong>: Fraction of requests
            returning 5xx errors</li>
            <li><strong>Measurement</strong>: Count of 5xx errors /
            total requests</li>
            <li><strong>Current</strong>: 0.05%</li>
            <li><strong>Target</strong>: &lt; 0.1% (SLO)</li>
            </ul>
            <h3 id="slos-service-level-objectives">SLOs (Service Level
            Objectives)</h3>
            <table>
            <thead>
            <tr class="header">
            <th>SLI</th>
            <th>SLO</th>
            <th>Error Budget</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>Availability</td>
            <td>99.9%</td>
            <td>0.1% = 43.2 min/month</td>
            </tr>
            <tr class="even">
            <td>P95 Latency</td>
            <td>&lt; 100ms</td>
            <td>&gt; 100ms for &gt; 0.1% requests</td>
            </tr>
            <tr class="odd">
            <td>P99 Latency</td>
            <td>&lt; 200ms</td>
            <td>&gt; 200ms for &gt; 0.1% requests</td>
            </tr>
            <tr class="even">
            <td>Error Rate</td>
            <td>&lt; 0.1%</td>
            <td>&gt; 0.1% for &gt; 0.1% requests</td>
            </tr>
            </tbody>
            </table>
            <h3 id="error-budget-policy">Error Budget Policy</h3>
            <pre class="mermaid"><code>graph LR
    Budget[Error Budget] --&gt; Check{Remaining?}
    Check --&gt;|&gt; 50%| Normal[Normal Operations]
    Check --&gt;|25-50%| Warning[Reduce Risky Changes]
    Check --&gt;|&lt; 25%| Critical[Stop Feature Work]
    Check --&gt;|0%| Emergency[Emergency Reliability Only]

    style Normal fill:#99ff99
    style Warning fill:#ffcc99
    style Critical fill:#ff9999
    style Emergency fill:#ff6666</code></pre>
            <p><strong>Policy</strong>: - <strong>&gt; 50%
            remaining</strong>: Normal operations, can ship features -
            <strong>25-50% remaining</strong>: Warning, reduce risky
            changes - <strong>&lt; 25% remaining</strong>: Critical,
            stop feature work, focus on reliability - <strong>0%
            remaining</strong>: Emergency, only reliability work, no new
            features</p>
            <hr />
            <h2 id="capacity-planning">Capacity Planning</h2>
            <h3 id="current-capacity">Current Capacity</h3>
            <p><strong>Per Region</strong>: - <strong>API
            Servers</strong>: 10 pods (can scale to 50) -
            <strong>CPU</strong>: 2 cores per pod = 20 cores (can scale
            to 100) - <strong>Memory</strong>: 4GB per pod = 40GB (can
            scale to 200GB) - <strong>QPS Capacity</strong>: ~10K QPS
            per region (can scale to 50K)</p>
            <p><strong>Total (3 Regions)</strong>: - <strong>QPS
            Capacity</strong>: ~30K QPS (can scale to 150K) -
            <strong>Target Load</strong>: 100K QPS peak -
            <strong>Headroom</strong>: 50% headroom at peak</p>
            <h3 id="scaling-strategy">Scaling Strategy</h3>
            <pre class="mermaid"><code>graph TD
    Load[Load] --&gt; Monitor[Monitor Metrics]
    Monitor --&gt; CPU{CPU &gt; 70%?}
    CPU --&gt;|Yes| ScaleUp[Scale Up Pods]
    CPU --&gt;|No| CheckMem{Memory &gt; 80%?}
    CheckMem --&gt;|Yes| ScaleUp
    CheckMem --&gt;|No| CheckLatency{Latency &gt; Threshold?}
    CheckLatency --&gt;|Yes| ScaleUp
    CheckLatency --&gt;|No| Maintain[Maintain Current]

    ScaleUp --&gt; Cooldown[Cooldown Period]
    Cooldown --&gt; Monitor

    style ScaleUp fill:#ffcc99
    style Maintain fill:#99ff99</code></pre>
            <p><strong>Auto-scaling Configuration</strong>: -
            <strong>Min replicas</strong>: 3 per region - <strong>Max
            replicas</strong>: 50 per region - <strong>Target
            CPU</strong>: 70% - <strong>Target memory</strong>: 80% -
            <strong>Scale up</strong>: Add 2 pods at a time -
            <strong>Scale down</strong>: Remove 1 pod at a time -
            <strong>Cooldown</strong>: 2 minutes</p>
            <h3 id="capacity-forecasting">Capacity Forecasting</h3>
            <p><strong>Growth Projection</strong>: -
            <strong>Current</strong>: 30K QPS average, 100K QPS peak -
            <strong>Growth</strong>: 20% per quarter - <strong>6
            months</strong>: ~43K QPS average, ~144K QPS peak -
            <strong>12 months</strong>: ~62K QPS average, ~207K QPS
            peak</p>
            <p><strong>Capacity Needs</strong>: - <strong>6
            months</strong>: Need to scale to ~150K QPS capacity -
            <strong>12 months</strong>: Need to scale to ~220K QPS
            capacity - <strong>Plan</strong>: Add 4th region at 6
            months, optimize at 12 months</p>
            <hr />
            <h2 id="failure-modes-blast-radius">Failure Modes &amp;
            Blast Radius</h2>
            <h3 id="failure-scenarios">Failure Scenarios</h3>
            <h4 id="scenario-1-single-region-failure">Scenario 1: Single
            Region Failure</h4>
            <p><strong>What fails</strong>: One region (e.g.,
            us-central1) goes down</p>
            <p><strong>Impact</strong>: - <strong>Blast radius</strong>:
            Users in that region - <strong>Availability</strong>: ~33%
            of users affected - <strong>Traffic</strong>: Rerouted to
            other regions - <strong>Latency</strong>: Increased for
            affected users</p>
            <p><strong>Detection</strong>: - Health checks fail - Error
            rate spikes - Latency increases</p>
            <p><strong>Recovery</strong>: 1. Load balancer automatically
            routes away from failed region 2. Scale up other regions to
            handle increased load 3. Investigate root cause 4. Restore
            failed region</p>
            <p><strong>Mitigation</strong>: - Multi-region deployment
            (already done) - Health checks (already configured) -
            Auto-scaling (already configured)</p>
            <h4 id="scenario-2-database-failure">Scenario 2: Database
            Failure</h4>
            <p><strong>What fails</strong>: Spanner becomes
            unavailable</p>
            <p><strong>Impact</strong>: - <strong>Blast radius</strong>:
            All regions, all users - <strong>Availability</strong>: 0%
            (complete outage) - <strong>Reads</strong>: Fail (cache
            helps for cached data) - <strong>Writes</strong>: Fail
            completely</p>
            <p><strong>Detection</strong>: - Database connection errors
            - Error rate spikes to 100% - All health checks fail</p>
            <p><strong>Recovery</strong>: 1. Spanner has automatic
            failover (multi-region) 2. Failover takes ~30 seconds 3.
            During failover: Serve cached data (reads only) 4. After
            failover: Resume normal operations</p>
            <p><strong>Mitigation</strong>: - Multi-region Spanner
            (already configured) - Read cache (reduces read load on DB)
            - Circuit breakers (prevent cascading failures)</p>
            <h4 id="scenario-3-cache-failure">Scenario 3: Cache
            Failure</h4>
            <p><strong>What fails</strong>: Redis cache becomes
            unavailable</p>
            <p><strong>Impact</strong>: - <strong>Blast radius</strong>:
            Affected region only - <strong>Availability</strong>: Still
            available (degraded) - <strong>Latency</strong>: Increases
            (no cache hits) - <strong>Database load</strong>: Increases
            significantly</p>
            <p><strong>Detection</strong>: - Cache connection errors -
            Latency increases - Database load increases</p>
            <p><strong>Recovery</strong>: 1. API continues without cache
            (degraded mode) 2. Scale up API servers to handle increased
            DB load 3. Restore cache 4. Warm cache with common data</p>
            <p><strong>Mitigation</strong>: - Cache is not critical path
            (system works without it) - Database can handle increased
            load (scaled appropriately) - Regional cache (failure
            isolated to one region)</p>
            <h4 id="scenario-4-load-balancer-failure">Scenario 4: Load
            Balancer Failure</h4>
            <p><strong>What fails</strong>: Global load balancer becomes
            unavailable</p>
            <p><strong>Impact</strong>: - <strong>Blast radius</strong>:
            All users, all regions - <strong>Availability</strong>: 0%
            (complete outage) - <strong>Traffic</strong>: Cannot reach
            API servers</p>
            <p><strong>Detection</strong>: - All health checks fail -
            Error rate spikes to 100% - No traffic reaching API
            servers</p>
            <p><strong>Recovery</strong>: 1. GCP load balancer has
            automatic failover 2. Failover takes ~1-2 minutes 3. After
            failover: Resume normal operations</p>
            <p><strong>Mitigation</strong>: - GCP managed service (high
            availability built-in) - Health checks (detect failures
            quickly) - Monitoring (alert on failures)</p>
            <h3 id="overload-scenarios">Overload Scenarios</h3>
            <h4 id="normal-load-300k-qps">10× Normal Load (300K
            QPS)</h4>
            <p><strong>Impact</strong>: - <strong>API Servers</strong>:
            Auto-scale to max (50 per region = 150 total) -
            <strong>Database</strong>: May see increased latency -
            <strong>Cache</strong>: May see cache misses increase -
            <strong>Latency</strong>: P95 may increase to 120ms, P99 to
            250ms</p>
            <p><strong>Mitigation</strong>: - Auto-scaling handles
            increased load - Cache reduces database load - Load shedding
            if needed (drop low-priority requests)</p>
            <h4 id="normal-load-3m-qps---ddos">100× Normal Load (3M QPS)
            - DDoS</h4>
            <p><strong>Impact</strong>: - <strong>API Servers</strong>:
            Overwhelmed even at max scale - <strong>Database</strong>:
            Overwhelmed - <strong>Availability</strong>: System fails -
            <strong>Latency</strong>: Extremely high or timeouts</p>
            <p><strong>Mitigation</strong>: - <strong>DDoS
            Protection</strong>: Cloud Armor (WAF) filters malicious
            traffic - <strong>Rate Limiting</strong>: Limit requests per
            IP/client - <strong>Load Shedding</strong>: Drop requests
            when overloaded - <strong>Circuit Breakers</strong>: Stop
            calling downstream services</p>
            <hr />
            <h2 id="observability">Observability</h2>
            <h3 id="metrics">Metrics</h3>
            <h4 id="service-metrics">Service Metrics</h4>
            <ul>
            <li><strong>Request rate</strong>: QPS per region, per
            endpoint</li>
            <li><strong>Latency</strong>: P50, P95, P99 per region, per
            endpoint</li>
            <li><strong>Error rate</strong>: 4xx, 5xx errors per region,
            per endpoint</li>
            <li><strong>Availability</strong>: Success rate per
            region</li>
            </ul>
            <h4 id="resource-metrics">Resource Metrics</h4>
            <ul>
            <li><strong>CPU</strong>: CPU utilization per pod, per
            region</li>
            <li><strong>Memory</strong>: Memory usage per pod, per
            region</li>
            <li><strong>Network</strong>: Bandwidth per region</li>
            <li><strong>Database</strong>: Spanner CPU, latency,
            QPS</li>
            <li><strong>Cache</strong>: Redis memory, hit rate,
            latency</li>
            </ul>
            <h4 id="business-metrics">Business Metrics</h4>
            <ul>
            <li><strong>Active users</strong>: Users per region</li>
            <li><strong>API usage</strong>: Endpoints usage, feature
            adoption</li>
            <li><strong>Error types</strong>: Breakdown of errors by
            type</li>
            </ul>
            <h3 id="dashboards">Dashboards</h3>
            <p><strong>Service Dashboard</strong>: - Request rate,
            latency, error rate - SLO compliance - Error budget
            remaining - Per-region breakdown</p>
            <p><strong>Capacity Dashboard</strong>: - Pod count, CPU,
            memory per region - Scaling events - Capacity utilization -
            Forecast vs actual</p>
            <p><strong>Database Dashboard</strong>: - Spanner QPS,
            latency - Connection pool usage - Query performance -
            Regional breakdown</p>
            <p><strong>Cache Dashboard</strong>: - Redis hit rate, miss
            rate - Memory usage - Latency - Per-region breakdown</p>
            <h3 id="logs">Logs</h3>
            <p><strong>Application Logs</strong>: - Request/response
            logs (structured JSON) - Error logs with stack traces -
            Performance logs (slow requests) - Request IDs for
            correlation</p>
            <p><strong>Infrastructure Logs</strong>: - GKE cluster logs
            - Load balancer access logs - Spanner query logs - Pub/Sub
            message logs</p>
            <h3 id="traces">Traces</h3>
            <p><strong>Distributed Tracing</strong>: - End-to-end
            request traces - Spans for: Load balancer → API → Cache →
            Database - Cross-region traces - Error traces with
            context</p>
            <h3 id="alerts">Alerts</h3>
            <p><strong>Critical Alerts</strong> (Page on-call): - SLO
            violation (availability &lt; 99.9%) - Error rate &gt; 1% -
            All regions down - Database unavailable</p>
            <p><strong>Warning Alerts</strong> (Notify, don’t page): -
            Error budget &lt; 50% - Latency &gt; threshold - Single
            region down - Cache failure</p>
            <p><strong>Info Alerts</strong> (Log only): - Scaling events
            - Deployment events - Capacity warnings</p>
            <hr />
            <h2 id="deployment-rollout-strategy">Deployment &amp;
            Rollout Strategy</h2>
            <h3 id="deployment-architecture">Deployment
            Architecture</h3>
            <pre class="mermaid"><code>graph LR
    Code[Code] --&gt; CI[CI/CD Pipeline]
    CI --&gt; Build[Build Container]
    Build --&gt; Registry[Container Registry]
    Registry --&gt; Canary[Canary Deployment]
    Canary --&gt; Prod[Production Deployment]

    Canary --&gt; Monitor[Monitor Metrics]
    Monitor --&gt;|Success| Prod
    Monitor --&gt;|Failure| Rollback[Rollback]

    style Canary fill:#ffcc99
    style Prod fill:#99ff99
    style Rollback fill:#ff9999</code></pre>
            <h3 id="rollout-process">Rollout Process</h3>
            <h4 id="phase-1-canary-5-traffic">Phase 1: Canary (5%
            traffic)</h4>
            <ol type="1">
            <li><strong>Deploy</strong>: Deploy new version to canary
            pods (5% of traffic)</li>
            <li><strong>Monitor</strong>: Monitor for 30 minutes
            <ul>
            <li>Check error rate, latency, SLO compliance</li>
            <li>Compare to baseline</li>
            </ul></li>
            <li><strong>Decision</strong>:
            <ul>
            <li><strong>Success</strong>: Proceed to Phase 2</li>
            <li><strong>Failure</strong>: Rollback immediately</li>
            </ul></li>
            </ol>
            <h4 id="phase-2-gradual-rollout-25-50-100">Phase 2: Gradual
            Rollout (25%, 50%, 100%)</h4>
            <ol type="1">
            <li><strong>25% traffic</strong>: Monitor for 15
            minutes</li>
            <li><strong>50% traffic</strong>: Monitor for 15
            minutes</li>
            <li><strong>100% traffic</strong>: Monitor for 1 hour</li>
            <li><strong>Complete</strong>: Mark rollout complete</li>
            </ol>
            <h3 id="rollback-strategy">Rollback Strategy</h3>
            <p><strong>Automatic Rollback</strong>: -
            <strong>Trigger</strong>: Error rate &gt; 1% OR latency &gt;
            threshold OR SLO violation - <strong>Action</strong>:
            Automatically rollback to previous version -
            <strong>Time</strong>: &lt; 2 minutes</p>
            <p><strong>Manual Rollback</strong>: -
            <strong>Trigger</strong>: Manual decision -
            <strong>Action</strong>: Rollback via CI/CD pipeline -
            <strong>Time</strong>: &lt; 5 minutes</p>
            <h3 id="feature-flags">Feature Flags</h3>
            <p><strong>Use Cases</strong>: - Risky features - A/B
            testing - Gradual feature rollout - Kill switches</p>
            <p><strong>Implementation</strong>: - Feature flags stored
            in Cloud Firestore - API checks flags on each request -
            Flags can be toggled without deployment</p>
            <hr />
            <h2 id="security">Security</h2>
            <h3 id="authentication-authorization">Authentication &amp;
            Authorization</h3>
            <p><strong>Authentication</strong>: -
            <strong>Method</strong>: OAuth 2.0 / JWT tokens -
            <strong>Provider</strong>: Google Identity Platform -
            <strong>Validation</strong>: Validate tokens on each
            request</p>
            <p><strong>Authorization</strong>: -
            <strong>Method</strong>: IAM policies + application-level
            checks - <strong>Principle</strong>: Least privilege -
            <strong>Audit</strong>: All access logged</p>
            <h3 id="data-protection">Data Protection</h3>
            <p><strong>Encryption</strong>: - <strong>At rest</strong>:
            Spanner encrypts data at rest (automatic) - <strong>In
            transit</strong>: TLS 1.3 for all connections -
            <strong>Keys</strong>: Managed by Cloud KMS</p>
            <p><strong>Data Classification</strong>: -
            <strong>PII</strong>: User data marked as PII -
            <strong>Retention</strong>: Data retained per policy -
            <strong>Deletion</strong>: Secure deletion procedures</p>
            <h3 id="network-security">Network Security</h3>
            <p><strong>VPC</strong>: - <strong>Private IPs</strong>: API
            servers use private IPs - <strong>Firewall rules</strong>:
            Restrict access to necessary ports -
            <strong>Peering</strong>: VPC peering for internal
            communication</p>
            <p><strong>DDoS Protection</strong>: - <strong>Cloud
            Armor</strong>: WAF and DDoS protection - <strong>Rate
            Limiting</strong>: Per-IP and per-client limits -
            <strong>Geographic filtering</strong>: Block known bad
            regions</p>
            <hr />
            <h2 id="incident-response-playbook">Incident Response
            Playbook</h2>
            <h3 id="incident-severity">Incident Severity</h3>
            <p><strong>P0 (Critical)</strong>: - Complete outage - SLO
            violation - Data loss - Security breach</p>
            <p><strong>P1 (High)</strong>: - Partial outage - Degraded
            performance - Error rate spike</p>
            <p><strong>P2 (Medium)</strong>: - Minor issues -
            Performance degradation - Non-critical errors</p>
            <h3 id="common-incidents">Common Incidents</h3>
            <h4 id="incident-1-high-error-rate">Incident 1: High Error
            Rate</h4>
            <p><strong>Symptoms</strong>: - Error rate &gt; 1% - 5xx
            errors increasing - Users reporting issues</p>
            <p><strong>Response</strong>: 1.
            <strong>Acknowledge</strong>: Acknowledge incident, create
            incident ticket 2. <strong>Assess</strong>: Check
            dashboards, identify affected region/endpoint 3.
            <strong>Mitigate</strong>: - Scale up if capacity issue -
            Rollback if deployment issue - Enable circuit breakers if
            downstream issue 4. <strong>Investigate</strong>: Root cause
            analysis 5. <strong>Resolve</strong>: Fix root cause 6.
            <strong>Postmortem</strong>: Write postmortem, action
            items</p>
            <p><strong>Runbook</strong>: [Link to detailed runbook]</p>
            <h4 id="incident-2-high-latency">Incident 2: High
            Latency</h4>
            <p><strong>Symptoms</strong>: - P95 latency &gt; 100ms - P99
            latency &gt; 200ms - Users reporting slowness</p>
            <p><strong>Response</strong>: 1.
            <strong>Acknowledge</strong>: Acknowledge incident 2.
            <strong>Assess</strong>: Check latency metrics, identify
            slow endpoints 3. <strong>Mitigate</strong>: - Scale up API
            servers - Check database performance - Check cache hit rate
            - Enable load shedding if needed 4.
            <strong>Investigate</strong>: Root cause analysis 5.
            <strong>Resolve</strong>: Fix root cause 6.
            <strong>Postmortem</strong>: Write postmortem</p>
            <p><strong>Runbook</strong>: [Link to detailed runbook]</p>
            <h4 id="incident-3-database-failure">Incident 3: Database
            Failure</h4>
            <p><strong>Symptoms</strong>: - Database connection errors -
            All writes failing - Error rate 100%</p>
            <p><strong>Response</strong>: 1.
            <strong>Acknowledge</strong>: Acknowledge incident (P0) 2.
            <strong>Assess</strong>: Check Spanner status, check
            failover status 3. <strong>Mitigate</strong>: - Spanner has
            automatic failover (~30 seconds) - Serve cached data for
            reads (if available) - Disable writes until database
            recovers 4. <strong>Investigate</strong>: Root cause
            analysis 5. <strong>Resolve</strong>: Database should
            recover automatically 6. <strong>Postmortem</strong>: Write
            postmortem</p>
            <p><strong>Runbook</strong>: [Link to detailed runbook]</p>
            <h3 id="on-call-rotation">On-Call Rotation</h3>
            <p><strong>Schedule</strong>: Weekly rotation
            <strong>Team</strong>: 3 engineers
            <strong>Escalation</strong>: - Level 1: On-call engineer -
            Level 2: Team lead - Level 3: Engineering manager</p>
            <p><strong>Tools</strong>: - PagerDuty for alerts - Slack
            for communication - Runbooks in wiki - Dashboards in
            Grafana</p>
            <hr />
            <h2 id="cost-optimization">Cost Optimization</h2>
            <h3 id="cost-breakdown">Cost Breakdown</h3>
            <p><strong>Monthly Costs</strong> (estimated): -
            <strong>GKE</strong>: $5,000 (compute, networking) -
            <strong>Spanner</strong>: $10,000 (storage, compute) -
            <strong>Memorystore</strong>: $2,000 (cache) - <strong>Load
            Balancer</strong>: $500 (traffic) -
            <strong>Pub/Sub</strong>: $500 (messaging) -
            <strong>Total</strong>: ~$18,000/month</p>
            <h3 id="optimization-strategies">Optimization
            Strategies</h3>
            <ol type="1">
            <li><strong>Right-sizing</strong>: Use appropriate instance
            sizes</li>
            <li><strong>Reserved instances</strong>: Use committed use
            discounts</li>
            <li><strong>Cache optimization</strong>: Increase cache hit
            rate</li>
            <li><strong>Database optimization</strong>: Optimize
            queries, use indexes</li>
            <li><strong>Auto-scaling</strong>: Scale down during low
            traffic</li>
            </ol>
            <hr />
            <h2 id="further-reading">Further Reading</h2>
            <p><strong>Comprehensive Guide</strong>: <a
            href="../further-reading/multi-region-api.html">Further
            Reading: Multi-Region API</a></p>
            <p><strong>Quick Links</strong>: - “Spanner: Google’s
            Globally-Distributed Database” (Corbett et al., 2012) - “The
            Datacenter as a Computer” (Barroso &amp; Hölzle, 2018) - <a
            href="https://cloud.google.com/architecture">Google Cloud
            Architecture Center</a> - <a
            href="../03-gcp-core-building-blocks/vpc-lb-dns.html">VPC, LB
            &amp; DNS</a> - <a
            href="../04-reliability-sre/sli-slo-error-budget.html">SLIs/SLOs</a>
            - <a href="../04-reliability-sre/prr-checklist.html">PRR
            Checklist</a> - <a
            href="../02-distributed-systems/overload-backpressure.html">Overload
            &amp; Backpressure</a> - <a href="README.html">Back to Case
            Studies</a></p>
            <hr />
            <h2 id="exercises">Exercises</h2>
            <ol type="1">
            <li><p><strong>Design improvements</strong>: How would you
            improve this design? What tradeoffs?</p></li>
            <li><p><strong>Handle new requirement</strong>: Add support
            for real-time notifications. How does this change the
            architecture?</p></li>
            <li><p><strong>Cost optimization</strong>: How would you
            reduce costs by 30%? What tradeoffs?</p></li>
            <li><p><strong>Disaster recovery</strong>: Design a disaster
            recovery plan. What’s the RTO? RPO?</p></li>
            </ol>
            <p><strong>Answer Key</strong>: <a
            href="../../exercises/answers/multi-region-api-answers.html">View
            Answers</a></p>
        </div>
    </main>
    <footer class="bg-gray-800 text-white text-center p-6 mt-16">
        <p>&copy; 2025 Data Engineering Guides. An illustrative web application.</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
      mermaid.initialize({ startOnLoad: true });
    </script>
</body>
</html>
