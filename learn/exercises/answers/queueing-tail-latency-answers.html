<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Answer Key: Queueing Theory & Tail Latency</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0KOVEMmgEkDaBLEqcMW5uudjLPMdWTocqpoLBTRPtcDGECroCvEOZE" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/learn.css">
</head>
<body class="antialiased">
    <header class="bg-white sticky top-0 z-50 shadow-md">
        <nav class="container mx-auto px-4 sm:px-6 py-4 flex justify-between items-center">
            <div class="text-xl sm:text-2xl font-bold text-gray-800">
                <a href="/index.html" class="flex items-center hover:text-gray-600">
                    <svg class="w-6 h-6 mr-2" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    <span>Data Engineering Concepts</span>
                </a>
            </div>
            <ul class="flex space-x-4 sm:space-x-6 text-gray-600 font-medium text-sm sm:text-base">
                <li><a href="/learn/index.html" class="hover:text-indigo-600 font-semibold">Back to Learn Topics</a></li>
                <li><a href="/aboutme.html" class="hover:text-[#bc5090] font-semibold">About Me</a></li>
            </ul>
        </nav>
    </header>
    <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-12">
        <div class="card">
            <div class="card-content">
                <h1 id="answer-key-queueing-theory-tail-latency">Answer Key: Queueing
Theory &amp; Tail Latency</h1>
<p><a
href="../../01-foundations/queueing-tail-latency.html#exercises">Back to
Exercises</a></p>
<hr />
<h2 id="exercise-1-calculate-capacity">Exercise 1: Calculate
Capacity</h2>
<p><strong>Question</strong>: Given arrival rate of 1000 req/s,
processing time of 20ms, target P99 of 200ms, how many servers do you
need?</p>
<h3 id="answer">Answer</h3>
<p><strong>Given</strong>: - Arrival rate (λ) = 1000 requests/second -
Processing time = 20ms = 0.02 seconds - Target P99 latency = 200ms = 0.2
seconds</p>
<p><strong>Approach</strong>: 1. Calculate service rate per server: μ =
1 / processing_time = 1 / 0.02 = 50 requests/second 2. For P99 latency,
we need to account for queueing delay 3. Target total time = 200ms =
processing_time + queue_wait_time 4. Max queue wait time = 200ms - 20ms
= 180ms = 0.18 seconds</p>
<p><strong>Using queueing theory</strong>: - For M/M/k queue, we need to
find k (number of servers) such that P99 wait time ≤ 0.18s - Utilization
per server: ρ = λ/(k×μ) = 1000/(k×50) = 20/k - We need ρ &lt; 1, so k
&gt; 20</p>
<p><strong>Approximation</strong>: - For low queueing delay, we want
utilization around 70-80% - At 70% utilization: k = 20/0.7 ≈ 29 servers
- At 80% utilization: k = 20/0.8 = 25 servers</p>
<p><strong>More precise calculation</strong>: - Using Erlang C formula
or simulation, for P99 wait time &lt; 180ms: - Need approximately
<strong>30-35 servers</strong> to meet P99 &lt; 200ms target</p>
<p><strong>Answer</strong>: <strong>30-35 servers</strong> (with 30%
headroom for safety)</p>
<p><strong>Key insights</strong>: - Queueing delay dominates tail
latency - Need significant headroom to meet P99 targets - Utilization
must be kept low (70-80%) for good tail latency</p>
<hr />
<h2 id="exercise-2-analyze-latency-distribution">Exercise 2: Analyze
Latency Distribution</h2>
<p><strong>Question</strong>: Given P50=10ms, P95=50ms, P99=200ms, what
can you infer about the system?</p>
<h3 id="answer-1">Answer</h3>
<p><strong>Given</strong>: - P50 (median) = 10ms - P95 = 50ms - P99 =
200ms</p>
<h3 id="analysis">Analysis</h3>
<p><strong>1. Tail Latency Ratio</strong>: - P99/P50 = 200/10 = 20× -
P95/P50 = 50/10 = 5×</p>
<p><strong>Inference</strong>: Very high tail latency ratio indicates: -
Significant queueing delay - High variability in request processing -
Possible head-of-line blocking - Resource contention issues</p>
<p><strong>2. Latency Distribution</strong>: - <strong>P50 =
10ms</strong>: Typical requests are fast (likely cache hits or simple
operations) - <strong>P95 = 50ms</strong>: 5% of requests take 5× longer
(likely cache misses, database queries) - <strong>P99 = 200ms</strong>:
1% of requests take 20× longer (likely complex operations, queueing, or
failures)</p>
<p><strong>3. System Characteristics</strong>: - <strong>Fast path
exists</strong>: P50 is low, suggesting many requests are optimized -
<strong>Slow path exists</strong>: P99 is very high, suggesting some
requests are expensive - <strong>High variability</strong>: Large gap
between P50 and P99 suggests inconsistent performance</p>
<p><strong>4. Likely Causes</strong>: - <strong>Head-of-line
blocking</strong>: Slow requests blocking fast ones - <strong>Cache
misses</strong>: Some requests miss cache and hit database -
<strong>Queueing</strong>: Requests waiting in queue during bursts -
<strong>Resource contention</strong>: CPU, memory, or I/O contention -
<strong>Long-tail operations</strong>: Some requests inherently slow
(complex queries, large payloads)</p>
<p><strong>5. Recommendations</strong>: - <strong>Separate fast and slow
paths</strong>: Use different queues or priorities - <strong>Improve
cache hit rate</strong>: Reduce cache misses - <strong>Optimize slow
operations</strong>: Identify and optimize P99 operations - <strong>Add
capacity</strong>: Reduce queueing delay - <strong>Implement request
prioritization</strong>: Prioritize fast requests</p>
<p><strong>Answer</strong>: The system has <strong>high tail latency
variability</strong> with a <strong>20× ratio</strong> between P50 and
P99, indicating: - Fast path exists (P50 = 10ms) - Slow path exists (P99
= 200ms) - Likely causes: head-of-line blocking, cache misses, queueing,
resource contention - Recommendations: separate fast/slow paths, improve
caching, optimize slow operations</p>
<hr />
<h2 id="exercise-3-design-queueing-strategy">Exercise 3: Design Queueing
Strategy</h2>
<p><strong>Question</strong>: Design a system that handles both fast
(1ms) and slow (100ms) requests without head-of-line blocking.</p>
<h3 id="answer-2">Answer</h3>
<p><strong>Problem</strong>: Head-of-line blocking occurs when slow
requests block fast ones in a single queue.</p>
<p><strong>Solution</strong>: Separate queues for fast and slow
requests.</p>
<h3 id="design">Design</h3>
<p><strong>1. Request Classification</strong>: - <strong>Fast
requests</strong>: Simple operations, cache hits, reads (1ms) -
<strong>Slow requests</strong>: Complex operations, cache misses, writes
(100ms)</p>
<p><strong>Classification methods</strong>: -
<strong>Endpoint-based</strong>: Different endpoints for fast/slow
operations - <strong>Request type</strong>: Read vs write, simple vs
complex - <strong>Estimated time</strong>: Classify based on operation
type - <strong>Dynamic</strong>: Measure request time, route to
appropriate queue</p>
<p><strong>2. Queue Architecture</strong>:</p>
<pre class="mermaid"><code>graph TD
    Requests[Incoming Requests] --&gt; Classify[Classify Request]
    Classify --&gt;|Fast| FastQueue[Fast Queue&lt;br/&gt;1ms processing]
    Classify --&gt;|Slow| SlowQueue[Slow Queue&lt;br/&gt;100ms processing]

    FastQueue --&gt; FastServers[Fast Servers&lt;br/&gt;High Priority]
    SlowQueue --&gt; SlowServers[Slow Servers&lt;br/&gt;Normal Priority]

    style FastQueue fill:#99ff99
    style SlowQueue fill:#ffcc99</code></pre>
<p><strong>3. Server Allocation</strong>: - <strong>Fast
servers</strong>: Dedicated servers for fast requests (high priority) -
<strong>Slow servers</strong>: Dedicated servers for slow requests
(normal priority) - <strong>Ratio</strong>: Allocate more servers to
fast queue (since fast requests are more common)</p>
<p><strong>4. Priority Scheduling</strong>: - <strong>Fast
queue</strong>: Higher priority, processed first - <strong>Slow
queue</strong>: Lower priority, processed when fast queue is empty -
<strong>Preemption</strong>: Fast requests can preempt slow ones (if
possible)</p>
<p><strong>5. Implementation Strategies</strong>:</p>
<p><strong>Option A: Separate Queues + Servers</strong> - Two separate
queues - Dedicated server pools - <strong>Pros</strong>: Complete
isolation, no blocking - <strong>Cons</strong>: Resource overhead,
complex routing</p>
<p><strong>Option B: Priority Queue</strong> - Single queue with
priorities - Fast requests have higher priority - <strong>Pros</strong>:
Simpler, efficient resource use - <strong>Cons</strong>: May still have
some blocking</p>
<p><strong>Option C: Work Stealing</strong> - Servers process from fast
queue first - Steal from slow queue when fast queue empty -
<strong>Pros</strong>: Efficient resource utilization -
<strong>Cons</strong>: More complex implementation</p>
<p><strong>6. Monitoring</strong>: - Track queue depth for each queue -
Monitor latency separately for fast/slow requests - Alert on queue depth
or latency thresholds</p>
<p><strong>Answer</strong>: <strong>Separate queues with priority
scheduling</strong>: 1. <strong>Classify requests</strong> into fast
(1ms) and slow (100ms) 2. <strong>Separate queues</strong>: Fast queue
(high priority) and slow queue (normal priority) 3. <strong>Dedicated
servers</strong>: More servers for fast queue 4. <strong>Priority
scheduling</strong>: Process fast queue first, then slow queue 5.
<strong>Monitor separately</strong>: Track metrics for each queue
independently</p>
<p><strong>Benefits</strong>: - No head-of-line blocking - Fast requests
get low latency - Slow requests don’t block fast ones - Better resource
utilization</p>
<p><strong>Tradeoffs</strong>: - More complex architecture - Need
request classification - Resource allocation decisions</p>
            </div>
        </div>
    </main>
    <footer class="bg-gray-800 text-white text-center p-6 mt-16">
        <p>&copy; 2025 Data Engineering Guides. An illustrative web application.</p>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
      mermaid.initialize({ startOnLoad: true });
    </script>
</body>
</html>
