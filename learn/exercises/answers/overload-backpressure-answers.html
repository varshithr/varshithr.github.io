<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Answer Key: Overload &amp; Backpressure</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0KOVEMmgEkDaBLEqcMW5uud
jLPMdWTocqpoLBTRPtcDGECroCvEOZE" crossorigin="anonymous">
    <link rel="stylesheet" href="../../learn/css/learn.css">
</head>
<body class="antialiased">
    <header class="bg-white sticky top-0 z-50 shadow-md">
        <nav class="container mx-auto px-4 sm:px-6 py-4 flex justify-between items-center">
            <div class="text-xl sm:text-2xl font-bold text-gray-800">
                <a href="../../index.html" class="flex items-center hover:text-gray-600">
                    <svg class="w-6 h-6 mr-2" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    <span>Data Engineering Concepts</span>
                </a>
            </div>
            <ul class="flex space-x-4 sm:space-x-6 text-gray-600 font-medium text-sm sm:text-base">
                <li><a href="../../aboutme.html" class="hover:text-[#bc5090] font-semibold">About Me</a></li>
            </ul>
        </nav>
    </header>
    <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-12">
        <div class="main-content">
            <h1 id="answer-key-overload-backpressure">Answer Key:
            Overload &amp; Backpressure</h1>
            <p><a
            href="../../02-distributed-systems/overload-backpressure.md#exercises">Back
            to Exercises</a></p>
            <hr />
            <h2 id="exercise-1-design-backpressure">Exercise 1: Design
            Backpressure</h2>
            <p><strong>Question</strong>: Design a system that handles
            10× load gracefully. What mechanisms do you use?</p>
            <h3 id="answer">Answer</h3>
            <p><strong>Goal</strong>: Handle 10× normal load without
            complete failure.</p>
            <h3 id="design-components">Design Components</h3>
            <p><strong>1. Load Detection</strong></p>
            <p><strong>Metrics to monitor</strong>: - Queue depth (alert
            when &gt; 2× normal) - Latency (alert when P95 &gt; 2×
            normal) - Error rate (alert when &gt; 2× normal) - Resource
            utilization (alert when &gt; 80%)</p>
            <p><strong>Detection thresholds</strong>: - <strong>Normal
            load</strong>: Baseline metrics - <strong>2× load</strong>:
            Warning threshold - <strong>5× load</strong>: Critical
            threshold - <strong>10× load</strong>: Emergency
            threshold</p>
            <p><strong>2. Auto-Scaling</strong></p>
            <p><strong>Horizontal scaling</strong>: - <strong>Min
            replicas</strong>: 3 (for redundancy) - <strong>Max
            replicas</strong>: 30 (10× normal capacity) - <strong>Scale
            up</strong>: Add 2 pods when CPU &gt; 70% or queue depth
            &gt; threshold - <strong>Scale down</strong>: Remove 1 pod
            when CPU &lt; 50% and queue depth &lt; threshold -
            <strong>Cooldown</strong>: 2 minutes between scaling
            events</p>
            <p><strong>Scaling strategy</strong>: - Scale aggressively
            (add capacity quickly) - Scale conservatively (remove
            capacity slowly) - Pre-scale for known traffic patterns</p>
            <p><strong>3. Load Shedding</strong></p>
            <p><strong>When to shed load</strong>: - Queue depth &gt;
            threshold (e.g., &gt; 1000 requests) - Latency &gt;
            threshold (e.g., P95 &gt; 500ms) - Resource utilization &gt;
            90%</p>
            <p><strong>Load shedding strategies</strong>: -
            <strong>Random drop</strong>: Drop random requests (simple
            but unfair) - <strong>Priority-based</strong>: Drop
            low-priority requests first (better UX) -
            <strong>Client-based</strong>: Drop requests from specific
            clients (protect important clients) - <strong>Request
            type</strong>: Drop read requests before write requests
            (preserve data integrity)</p>
            <p><strong>Implementation</strong>: - Drop requests at load
            balancer or API gateway - Return HTTP 429 (Too Many
            Requests) - Include Retry-After header</p>
            <p><strong>4. Circuit Breakers</strong></p>
            <p><strong>Purpose</strong>: Stop calling downstream
            services when they’re failing.</p>
            <p><strong>Configuration</strong>: - <strong>Failure
            threshold</strong>: 50% failure rate -
            <strong>Timeout</strong>: 5 seconds - <strong>Half-open
            interval</strong>: 30 seconds - <strong>Success
            threshold</strong>: 3 successful requests</p>
            <p><strong>Behavior</strong>: - <strong>Closed</strong>:
            Normal operation, calls downstream - <strong>Open</strong>:
            Fails fast, doesn’t call downstream -
            <strong>Half-open</strong>: Test if downstream recovered</p>
            <p><strong>5. Rate Limiting</strong></p>
            <p><strong>Per-client rate limits</strong>: - Normal
            clients: 100 requests/second - Premium clients: 1000
            requests/second - Anonymous clients: 10 requests/second</p>
            <p><strong>Global rate limits</strong>: - Total system
            capacity: 10,000 QPS - When exceeded: Return 429, queue
            requests</p>
            <p><strong>6. Graceful Degradation</strong></p>
            <p><strong>Feature flags</strong>: - Disable non-critical
            features under load - Reduce functionality to core features
            only - Return cached data instead of fresh data</p>
            <p><strong>Response strategies</strong>: - <strong>Fast
            path</strong>: Serve cached data, skip expensive operations
            - <strong>Reduced functionality</strong>: Disable optional
            features - <strong>Timeout reduction</strong>: Reduce
            timeouts to fail fast</p>
            <p><strong>7. Monitoring &amp; Alerting</strong></p>
            <p><strong>Metrics</strong>: - Request rate, queue depth,
            latency, error rate - Resource utilization (CPU, memory,
            I/O) - Scaling events, load shedding events</p>
            <p><strong>Alerts</strong>: - <strong>Warning</strong>: 2×
            load detected - <strong>Critical</strong>: 5× load detected
            - <strong>Emergency</strong>: 10× load detected</p>
            <h3 id="complete-design">Complete Design</h3>
            <pre class="mermaid"><code>graph TD
    Load[10× Load] --&gt; Detect[Load Detection]
    Detect --&gt; Scale{Auto-Scale?}
    Scale --&gt;|Yes| ScaleUp[Scale Up]
    Scale --&gt;|No| Shed{Load Shed?}
    Shed --&gt;|Yes| Drop[Drop Requests]
    Shed --&gt;|No| Circuit{Circuit Break?}
    Circuit --&gt;|Yes| FailFast[Fail Fast]
    Circuit --&gt;|No| Degrade[Graceful Degradation]

    ScaleUp --&gt; Monitor[Monitor]
    Drop --&gt; Monitor
    FailFast --&gt; Monitor
    Degrade --&gt; Monitor

    style Load fill:#ff9999
    style Detect fill:#ffcc99
    style Monitor fill:#99ff99</code></pre>
            <p><strong>Answer</strong>: <strong>Multi-layered
            approach</strong>:</p>
            <ol type="1">
            <li><strong>Auto-scaling</strong>: Scale to 10× capacity (30
            replicas)</li>
            <li><strong>Load shedding</strong>: Drop low-priority
            requests when queue depth &gt; threshold</li>
            <li><strong>Circuit breakers</strong>: Stop calling failing
            downstream services</li>
            <li><strong>Rate limiting</strong>: Limit per-client and
            global rates</li>
            <li><strong>Graceful degradation</strong>: Disable
            non-critical features</li>
            <li><strong>Monitoring</strong>: Track metrics and alert on
            thresholds</li>
            </ol>
            <p><strong>Key principles</strong>: - <strong>Fail
            gracefully</strong>: Better to serve some requests than none
            - <strong>Protect core</strong>: Preserve critical
            functionality - <strong>Fail fast</strong>: Don’t wait
            indefinitely - <strong>Monitor everything</strong>: Know
            what’s happening</p>
            <hr />
            <h2 id="exercise-2-prevent-cascades">Exercise 2: Prevent
            Cascades</h2>
            <p><strong>Question</strong>: Service A calls Service B. How
            do you prevent B’s failure from cascading to A?</p>
            <h3 id="answer-1">Answer</h3>
            <p><strong>Problem</strong>: When Service B fails, Service A
            may also fail, creating a cascade.</p>
            <h3 id="prevention-mechanisms">Prevention Mechanisms</h3>
            <p><strong>1. Circuit Breaker</strong></p>
            <p><strong>Purpose</strong>: Stop calling Service B when
            it’s failing.</p>
            <p><strong>Configuration</strong>: - <strong>Failure
            threshold</strong>: 50% failure rate over 10 requests -
            <strong>Timeout</strong>: 2 seconds (fail fast) -
            <strong>Half-open interval</strong>: 30 seconds -
            <strong>Success threshold</strong>: 3 successful requests to
            close circuit</p>
            <p><strong>Behavior</strong>: - <strong>Closed</strong>:
            Normal operation, calls Service B - <strong>Open</strong>:
            Fails fast, returns error immediately (doesn’t call Service
            B) - <strong>Half-open</strong>: Test if Service B
            recovered</p>
            <p><strong>Implementation</strong>:</p>
            <div class="sourceCode" id="cb2"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> circuit_breaker.is_open():</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> error(<span class="st">&quot;Service B unavailable&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> call_service_b()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        circuit_breaker.record_success()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        circuit_breaker.record_failure()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span></span></code></pre></div>
            <p><strong>2. Timeouts</strong></p>
            <p><strong>Purpose</strong>: Don’t wait indefinitely for
            Service B.</p>
            <p><strong>Configuration</strong>: - <strong>Connection
            timeout</strong>: 1 second - <strong>Request
            timeout</strong>: 2 seconds - <strong>Total
            timeout</strong>: 3 seconds</p>
            <p><strong>Why important</strong>: - Prevents Service A from
            hanging - Fails fast instead of waiting - Reduces resource
            usage</p>
            <p><strong>3. Retry Limits</strong></p>
            <p><strong>Purpose</strong>: Limit retries to prevent
            amplifying load on Service B.</p>
            <p><strong>Configuration</strong>: - <strong>Max
            retries</strong>: 2 (total 3 attempts) - <strong>Exponential
            backoff</strong>: 100ms, 200ms, 400ms - <strong>Retry only
            on</strong>: Transient errors (5xx, timeouts) -
            <strong>Don’t retry on</strong>: Client errors (4xx)</p>
            <p><strong>Why important</strong>: - Prevents retry storms -
            Reduces load on failing service - Fails fast after retries
            exhausted</p>
            <p><strong>4. Bulkhead Pattern</strong></p>
            <p><strong>Purpose</strong>: Isolate failures to prevent
            resource exhaustion.</p>
            <p><strong>Implementation</strong>: - <strong>Separate
            thread pools</strong>: One for Service B calls, one for
            other operations - <strong>Resource limits</strong>: Limit
            resources used for Service B calls -
            <strong>Isolation</strong>: Failure in Service B doesn’t
            affect other operations</p>
            <p><strong>5. Fallback Mechanisms</strong></p>
            <p><strong>Purpose</strong>: Provide alternative behavior
            when Service B fails.</p>
            <p><strong>Options</strong>: - <strong>Cached data</strong>:
            Return cached response from Service B - <strong>Default
            values</strong>: Return sensible defaults - <strong>Degraded
            mode</strong>: Reduce functionality but continue operating -
            <strong>Error response</strong>: Return error but don’t
            crash</p>
            <p><strong>6. Load Shedding</strong></p>
            <p><strong>Purpose</strong>: Reduce load on Service A to
            prevent cascade.</p>
            <p><strong>When</strong>: Service B is failing, reduce load
            on Service A: - Drop low-priority requests - Reduce request
            rate - Return errors for non-critical requests</p>
            <p><strong>7. Monitoring &amp; Alerting</strong></p>
            <p><strong>Purpose</strong>: Detect failures early.</p>
            <p><strong>Metrics</strong>: - Service B error rate -
            Service B latency - Circuit breaker state - Retry rate</p>
            <p><strong>Alerts</strong>: - Service B error rate &gt;
            threshold - Circuit breaker opened - High retry rate</p>
            <h3 id="complete-solution">Complete Solution</h3>
            <pre class="mermaid"><code>sequenceDiagram
    participant A as Service A
    participant CB as Circuit Breaker
    participant B as Service B

    A-&gt;&gt;CB: Check circuit state
    alt Circuit Open
        CB--&gt;&gt;A: Fail fast (don&#39;t call B)
    else Circuit Closed
        A-&gt;&gt;B: Call with timeout
        alt Success
            B--&gt;&gt;A: Response
            A-&gt;&gt;CB: Record success
        else Failure
            B--&gt;&gt;A: Error/Timeout
            A-&gt;&gt;CB: Record failure
            alt Retry limit not exceeded
                A-&gt;&gt;B: Retry with backoff
            else Retry limit exceeded
                A-&gt;&gt;CB: Open circuit
                A-&gt;&gt;A: Use fallback
            end
        end
    end</code></pre>
            <p><strong>Answer</strong>: <strong>Multi-layered
            defense</strong>:</p>
            <ol type="1">
            <li><strong>Circuit breaker</strong>: Stop calling Service B
            when it’s failing</li>
            <li><strong>Timeouts</strong>: Don’t wait indefinitely (fail
            fast)</li>
            <li><strong>Retry limits</strong>: Limit retries (max 2
            retries)</li>
            <li><strong>Exponential backoff</strong>: Space out
            retries</li>
            <li><strong>Bulkhead</strong>: Isolate Service B calls
            (separate thread pool)</li>
            <li><strong>Fallback</strong>: Use cached data or defaults
            when Service B fails</li>
            <li><strong>Load shedding</strong>: Reduce load on Service A
            if Service B failing</li>
            <li><strong>Monitoring</strong>: Alert on Service B
            failures</li>
            </ol>
            <p><strong>Key principles</strong>: - <strong>Fail
            fast</strong>: Don’t wait for failing service -
            <strong>Isolate failures</strong>: Prevent resource
            exhaustion - <strong>Provide fallbacks</strong>: Continue
            operating when possible - <strong>Monitor
            everything</strong>: Detect failures early</p>
            <hr />
            <h2 id="exercise-3-load-shedding-strategy">Exercise 3: Load
            Shedding Strategy</h2>
            <p><strong>Question</strong>: Design a load shedding
            strategy for an API that handles both read and write
            requests. Which requests do you drop first?</p>
            <h3 id="answer-2">Answer</h3>
            <p><strong>Goal</strong>: Drop requests when overloaded,
            prioritizing important requests.</p>
            <h3 id="request-classification">Request Classification</h3>
            <p><strong>1. Request Types</strong>: - <strong>Read
            requests</strong>: GET requests, data retrieval -
            <strong>Write requests</strong>: POST, PUT, DELETE requests,
            data modification</p>
            <p><strong>2. Request Priority</strong>: - <strong>Critical
            writes</strong>: Must succeed (payments, orders) -
            <strong>Normal writes</strong>: Should succeed (updates,
            creates) - <strong>Critical reads</strong>: Must succeed
            (auth checks, critical data) - <strong>Normal
            reads</strong>: Can be dropped (cached data,
            non-critical)</p>
            <p><strong>3. Client Classification</strong>: -
            <strong>Premium clients</strong>: High priority -
            <strong>Normal clients</strong>: Normal priority -
            <strong>Anonymous clients</strong>: Low priority</p>
            <h3 id="load-shedding-strategy">Load Shedding Strategy</h3>
            <p><strong>Priority Order</strong> (drop in this order):</p>
            <p><strong>1. Anonymous read requests</strong> (lowest
            priority) - <strong>Why</strong>: Anonymous users, can retry
            - <strong>Impact</strong>: Low (users can retry) -
            <strong>When</strong>: Drop when queue depth &gt; 500</p>
            <p><strong>2. Normal read requests from normal
            clients</strong> - <strong>Why</strong>: Can be cached, less
            critical - <strong>Impact</strong>: Medium (users may see
            stale data) - <strong>When</strong>: Drop when queue depth
            &gt; 1000</p>
            <p><strong>3. Normal write requests from normal
            clients</strong> - <strong>Why</strong>: Less critical than
            reads, can retry - <strong>Impact</strong>: Medium (users
            may need to retry) - <strong>When</strong>: Drop when queue
            depth &gt; 1500</p>
            <p><strong>4. Critical read requests from normal
            clients</strong> - <strong>Why</strong>: Important but can
            retry - <strong>Impact</strong>: High (users may be
            affected) - <strong>When</strong>: Drop when queue depth
            &gt; 2000</p>
            <p><strong>5. Normal write requests from premium
            clients</strong> - <strong>Why</strong>: Premium clients but
            non-critical writes - <strong>Impact</strong>: High (premium
            users affected) - <strong>When</strong>: Drop when queue
            depth &gt; 2500</p>
            <p><strong>6. Critical write requests from normal
            clients</strong> - <strong>Why</strong>: Critical but from
            normal clients - <strong>Impact</strong>: Very high
            (critical operations affected) - <strong>When</strong>: Drop
            when queue depth &gt; 3000</p>
            <p><strong>7. Critical read requests from premium
            clients</strong> - <strong>Why</strong>: Critical and
            premium - <strong>Impact</strong>: Very high -
            <strong>When</strong>: Drop when queue depth &gt; 3500</p>
            <p><strong>8. Critical write requests from premium
            clients</strong> (never drop) - <strong>Why</strong>: Most
            critical, never drop - <strong>Impact</strong>: Critical
            (payments, orders) - <strong>When</strong>: Never drop
            (serve even if overloaded)</p>
            <h3 id="implementation">Implementation</h3>
            <p><strong>Load shedding algorithm</strong>:</p>
            <div class="sourceCode" id="cb4"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> should_drop_request(request):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    queue_depth <span class="op">=</span> get_queue_depth()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Never drop critical writes from premium clients</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> request.is_critical_write() <span class="kw">and</span> request.is_premium():</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Drop based on priority and queue depth</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    priority <span class="op">=</span> request.get_priority()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    threshold <span class="op">=</span> get_threshold_for_priority(priority)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> queue_depth <span class="op">&gt;</span> threshold</span></code></pre></div>
            <p><strong>Thresholds</strong>: - Anonymous reads: 500 -
            Normal reads: 1000 - Normal writes: 1500 - Critical reads:
            2000 - Premium normal writes: 2500 - Critical writes: 3000 -
            Premium critical reads: 3500 - Premium critical writes:
            Never</p>
            <h3 id="response-strategy">Response Strategy</h3>
            <p><strong>When dropping requests</strong>: - Return HTTP
            429 (Too Many Requests) - Include Retry-After header (e.g.,
            5 seconds) - Include error message explaining why - Log
            dropped requests for analysis</p>
            <p><strong>Monitoring</strong>: - Track dropped requests by
            type - Monitor queue depth - Alert on high drop rate -
            Analyze drop patterns</p>
            <h3 id="answer-3">Answer</h3>
            <p><strong>Load shedding priority</strong> (drop in
            order):</p>
            <ol type="1">
            <li><strong>Anonymous read requests</strong> (drop at queue
            depth &gt; 500)</li>
            <li><strong>Normal read requests</strong> (drop at queue
            depth &gt; 1000)</li>
            <li><strong>Normal write requests</strong> (drop at queue
            depth &gt; 1500)</li>
            <li><strong>Critical read requests</strong> (drop at queue
            depth &gt; 2000)</li>
            <li><strong>Premium normal write requests</strong> (drop at
            queue depth &gt; 2500)</li>
            <li><strong>Critical write requests</strong> (drop at queue
            depth &gt; 3000)</li>
            <li><strong>Premium critical read requests</strong> (drop at
            queue depth &gt; 3500)</li>
            <li><strong>Premium critical write requests</strong>
            (<strong>never drop</strong>)</li>
            </ol>
            <p><strong>Key principles</strong>: - <strong>Preserve data
            integrity</strong>: Never drop critical writes -
            <strong>Prioritize premium clients</strong>: Drop normal
            clients first - <strong>Reads before writes</strong>: Drop
            reads before writes (writes are harder to retry) -
            <strong>Gradual degradation</strong>: Drop less important
            requests first - <strong>Monitor and adjust</strong>: Track
            drops and adjust thresholds</p>
            <p><strong>Rationale</strong>: - <strong>Writes are harder
            to retry</strong>: Users may have already acted -
            <strong>Reads can be cached</strong>: Stale data is better
            than no data - <strong>Premium clients pay more</strong>:
            Should get better service - <strong>Critical
            operations</strong>: Must succeed (payments, orders)</p>
        </div>
    </main>
    <footer class="bg-gray-800 text-white text-center p-6 mt-16">
        <p>&copy; 2025 Data Engineering Guides. An illustrative web application.</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
      mermaid.initialize({ startOnLoad: true });
    </script>
</body>
</html>
